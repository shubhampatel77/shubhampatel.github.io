---
title: 'Personalized Image Editing and Content Addition using Diffusion Models'
date: 2024-07-16
permalink: /posts/2024/07/blog-post-1/
tags:
  - Diffusion Models
  - text-to-image generation
  - personalized image editing
---

Hey there! Ever wished you could magically add your favorite person (or pet!) into your favourite scenic photo without it looking like a bad Photoshop job? üåü Well, you're in luck! In this post, I'll walk you through an exciting project I worked on where I used cutting-edge diffusion models to personalize image editing, all done in a generative fashion. Get ready to dive into the fascinating world of AI-driven image manipulation!
Before we jump into the nitty-gritty of what I did, let's take a quick tour of the AI neighborhood. Recent advancements in image synthesis and editing, thanks to the power of diffusion models, have opened up a whole new world of possibilities. These models have been game-changers in generating high-quality and diverse images, making personalization and fine-grained editing more achievable than ever. Think of it like giving an artist a magical brush that never runs out of paint and always knows exactly what you want. üñåÔ∏è‚ú®

So, what's the deal with diffusion models? Imagine trying to clean a dirty window‚Äîdiffusion models attempt to generate images from pure noise. They start with a noisy, unclear image and gradually denoise it to reveal a crystal-clear picture. These models are called Denoising Diffusion Probabilistic Models (DDPMs). In the forward process, we gradually add Gaussian noise to an image $x_0$ over a series of timesteps $t$. The noisy image at each step $t$ is represented as \\( x_t \\):

$$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I) $$

where $\beta_{t}$ are the variance terms controlling the noise level at each timestep.
The magic happens in the reverse process, where the model learns to denoise the image step-by-step, reconstructing the original image from the noisy version:

$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$

Here, $ \mu_\theta(x_t, t)$ and $ \Sigma_\theta(x_t, t) $ are parameters learned by the model to predict the mean and variance of the denoised image. Check the full [report](https://github.com/shubhampatel77/shubhampatel77.github.io/blob/master/files/CS_6384_Project_Final_Report.pdf) for more details

## Personalized Image Generation using a single image
 
Here's where things get really cool. Personalized image generation means tailoring images to match specific characteristics. Tools like DreamBooth fine-tune requires 3-5 images, binding your subject to a unique identifying token like, say "ctblk" (using the consonants of cat black) and "frstx" for your given photo of a forest. This way, by simply writing a text prompt like "A photo of a ctblk cat in a frstx forst", you can generate images of your favorite subject in your favourite scenes, making them look just as amazing as you envisioned. Think of it as teaching your AI model to recognize and replicate the uniqueness of your subject and your background image just like a true artist. üé®üì∏
Now, let‚Äôs talk about single image editing. We cannot expect the user to provide multiple, diverse images of the subject and/or the object. So how do we achieve the same result with a single image? Initial experimentation yielded in reasonable images generated. This was achieved by carefully selecting the learning rate and the number of training steps to make sure the model knows just enough to produce your subject (called the instance from now on) and scene (called the target from now on). üê±‚òïÔ∏è

## Method

Alright, let's get down to the fun part‚Äîhow does it all work?

### Stage I: Target Image Fine-Tuning

First up, we fine-tune our model with a target image (favourite scene). Similarly, we start with a scene, like a mountain or a beach, and train our model to understand it in detail. This helps the model get the lay of the land‚Äîquite literally! The loss function below may seem quite involved but it is simply the MSE loss between the actual noise added to the image and the noise predicted by our model. The second term helps to make sure the model does not forget how to make other mountains or beaches and does not overfit to user-provided image. It is called the prior-preservation loss:

$$ L_{\text{target}} = \mathbb{E}_{(x,c),\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta(z_t, t, \tau_\theta(c)) \|_2^2 \right] + \lambda \mathbb{E}_{(x_{\text{pr}}, c_{\text{pr}}),\epsilon \sim \mathcal{N}(0,1),t} \left[ \| \epsilon - \epsilon_\theta(z_{\text{pr}}, t, \tau_\theta(c_{\text{pr}})) \|_2^2 \right] $$

where $\epsilon$ is the noise vector, $z_t$ is the latent code at timestep $ t $, and $\tau_\theta(c)$ and $ \tau_\theta(c_{\text{pr}}) $ are the text embeddings for the instance and general content, respectively.


### Stage II: Instance Image Fine-Tuning

Next, I introduce our star‚Äîthe instance image. This could be a person, a pet, or even your favorite gadget. We fine-tune the model again, but this time, it learns to integrate this instance into the previously learned scenes seamlessly. The loss term is the same. For further details, please check the full [report](/files/CS_6384_Project_Final_Report.pdf) 
![Method](/images/method.png)  

## Experiments

Let's get experimental! Here‚Äôs how I tested our method. I used a set of eight images, organized into pairs of targets (scenes) and instances. For example, a mountain paired with a backpack, or a beach paired with a person. This diverse mix helped me ensure the model could handle all sorts of combinations‚Äîkind of like mixing and matching outfits for different occasions.

## Results

My models generated some pretty interesting results! I created images that seamlessly blended the target and instance. For example, a backpack perfectly integrated into a mountain scene, or a person walking along a beach. While simpler instances like a cat turned out purr-fectly, more complex instances like human faces needed a bit more tweaking. Here‚Äôs a sneak peek:

![Generated Images](/images/results.png)  
*Figure 2: Generated images per prompt per model, showing the integration of instances into target scenes.*

### Evaluation Metrics

To measure how well the models performed, I used two main metrics: Learned Perceptual Image Patch Similarity (LPIPS) and CLIP-T scores. LPIPS helped me gauge the perceptual similarity between the generated and input images, while CLIP-T scores assessed the semantic accuracy of the image-caption pairs. Basically, I wanted to ensure the generated images not only looked right but also made sense in context.

*Insert LPIPS and CLIP-T score explanations and results here.*

### Hyperparameter Optimization

Finding the perfect balance for our model settings was crucial. I experimented with different learning rates and step counts to optimize performance. For instance, inanimate objects like a backpack required a specific set of parameters, while detailed human faces needed a different approach. It‚Äôs like adjusting your recipe to get the perfect bake‚Äîsometimes you need a little more sugar, sometimes a little less.

### Further Experiments with SINE Framework

While the results may look good, they still lack diversity and a perfect one-to-one correspondence with user given images. I believe this is because the model does not have enough context based on only a single image. Future work will explore the SINE framework, which utilized patch-based fine-tuning which decouples the pixel position with its corresponding concept, essentially giving more context about the subject from a single image. Further experimentation with the order of fine-tuning and hyperparameter tuning will also be done.

## Conclusion

In a nutshell, this project has taken us on an exciting journey through the world of image editing using diffusion models. We saw a method to seamlessly integrate subjects into various scenes, enhancing the model‚Äôs ability to generate personalized, high-fidelity images. While significant strides have been made, there‚Äôs always room for improvement, especially in achieving perfect fidelity in complex scenarios. Future work will focus on refining our fine-tuning strategies and exploring new techniques to push the boundaries of what‚Äôs possible in AI-driven image editing.

Stay tuned for more updates, and feel free to experiment with your own images! Who knows, the next breakthrough could be just around the corner. üöÄ‚ú®


### Implementation Details

All experiments were conducted on a A6000 cloud-based GPU. Each pair of images was used to train a separate model. The settings varied depending on the complexity of the scene and the details of the instance. Think of it as customizing your training plan based on whether you're preparing for a sprint or a marathon. üèÉ‚Äç‚ôÇÔ∏èüí®
